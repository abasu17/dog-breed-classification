{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Classification based on RESNET-50(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole project is designed using the following steps:<br><br>\n",
    "<ol>1. Perform EDA to know the Data</ol>\n",
    "<ol>2. Handling unbalance dataset</ol>\n",
    "<ol>3. Creation of the Model</ol>\n",
    "<ol>4. Training the model</ol>\n",
    "<ol>5. Test the model</ol>\n",
    "<ol>6. Calculate the model metrics</ol>\n",
    "<ol>7. Prediction</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "\n",
    "# for split dataset into train, test & validation\n",
    "import splitfolders\n",
    "\n",
    "# for creating model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# for prevention of model overfit\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# for data generation\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for testing\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# for model scoring & metric\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the CSV\n",
    "labels_data = pd.read_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10222</td>\n",
       "      <td>10222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10222</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>d265584fd8255cf0ffb477cdfddd32f9</td>\n",
       "      <td>scottish_deerhound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id               breed\n",
       "count                              10222               10222\n",
       "unique                             10222                 120\n",
       "top     d265584fd8255cf0ffb477cdfddd32f9  scottish_deerhound\n",
       "freq                                   1                 126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base path\n",
    "TRAIN_DATA = \"./train/\"\n",
    "INPUT_PATH = \"./temporary_input_data/\"\n",
    "OUTPUT_DATASET = \"./input_data\"\n",
    "TESING_DATASET = \"./input_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOLING = \"avg\"\n",
    "WEIGHTS = \"imagenet\"\n",
    "\n",
    "CLASSES = None\n",
    "DENSE_ACTIVATION = \"softmax\"\n",
    "\n",
    "OBJ_FUNCTION = \"categorical_crossentropy\"\n",
    "LOSS_METRICS = ['accuracy']\n",
    "\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "TRAINING_BATCH_SIZE = 100\n",
    "\n",
    "STEPS_PER_EPOCH_TRAINING = 10\n",
    "NUM_EPOCHS = 10\n",
    "STEPS_PER_EPOCH_VALIDATION = 10\n",
    "\n",
    "TESTING_BATCH_SIZE = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<U><B>Point No. 1</B></U><br>\n",
    "According to the statement <B>\"The classifier should only predict scores for these breeds : beagle, chihuahua, doberman, french_bulldog, golden_retriever, malamute, pug, saint_bernard, scottish_deerhound, tibetan_mastiff.\"</B>\n",
    "<br>\n",
    "So we are creating a list of the dogs name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = [\n",
    "    \"beagle\", \n",
    "    \"chihuahua\", \n",
    "    \"doberman\", \n",
    "    \"french_bulldog\", \n",
    "    \"golden_retriever\", \n",
    "    \"malamute\", \n",
    "    \"pug\", \n",
    "    \"saint_bernard\", \n",
    "    \"scottish_deerhound\", \n",
    "    \"tibetan_mastiff\"]\n",
    "\n",
    "CLASSES = len(selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.local/lib/python3.6/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beagle': 105,\n",
       " 'chihuahua': 71,\n",
       " 'doberman': 74,\n",
       " 'french_bulldog': 70,\n",
       " 'golden_retriever': 67,\n",
       " 'malamute': 81,\n",
       " 'pug': 94,\n",
       " 'saint_bernard': 84,\n",
       " 'scottish_deerhound': 126,\n",
       " 'tibetan_mastiff': 69}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of each label\n",
    "labels_data[labels_data.breed.isin(selected_labels)].groupby([\"breed\"]).agg(len).T.to_dict(\"r\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the minimum count from the list\n",
    "minimum_count = labels_data[labels_data.breed.isin(selected_labels)].groupby([\"breed\"]).agg(len).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Unbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate each of the labels from above list\n",
    "for each_label in selected_labels:\n",
    "    \n",
    "    # list all the images name have the present label\n",
    "    images = list(labels_data[labels_data[\"breed\"] == each_label].head(minimum_count.id).id)\n",
    "    \n",
    "    # iterate each of the image from the list\n",
    "    for image_id in images:\n",
    "        \n",
    "        # set the source image \n",
    "        image_src = TRAIN_DATA + image_id + \".jpg\"\n",
    "        \n",
    "        # set the destination path\n",
    "        image_dst = INPUT_PATH + each_label + \"/\"\n",
    "        \n",
    "        # check if folder not exist with label name\n",
    "        if not os.path.exists(image_dst):\n",
    "            \n",
    "            # create the folder\n",
    "            os.makedirs(image_dst)\n",
    "        \n",
    "        # copy the image into the folder\n",
    "        copyfile(image_src, image_dst + \"/\" + image_id + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 670 files [00:00, 4591.47 files/s]\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train, test & validation\n",
    "splitfolders.ratio(INPUT_PATH, OUTPUT_DATASET, seed=42, ratio=(.6, .2, .2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<U><B>Point No. 3</B></U><br>\n",
    "The classifier should only be built using <B>Resnet50</B> CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Sequential object\n",
    "model = Sequential()\n",
    "\n",
    "# add ResNet50 arch into the layer array\n",
    "model.add(ResNet50(include_top = True, pooling = POOLING, weights = WEIGHTS))\n",
    "\n",
    "# add prediction layer\n",
    "model.add(Dense(CLASSES, activation = DENSE_ACTIVATION))\n",
    "\n",
    "# customize trainable parameter\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 1000)              25636712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 25,646,722\n",
      "Trainable params: 10,010\n",
      "Non-trainable params: 25,636,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# view model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Stochastic Gradient Descent as Optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer = sgd, loss = OBJ_FUNCTION, metrics = LOSS_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    './input_data/train',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=TRAINING_BATCH_SIZE,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# create validation data\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    './input_data/val',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=TRAINING_BATCH_SIZE,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create early_stopper to prevent overfit.\n",
    "early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE)\n",
    "\n",
    "# create checkpointer for store the model into a file\n",
    "checkpointer = ModelCheckpoint(filepath = 'new_model.hdf5', monitor = 'val_loss', save_best_only = True, mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 4/10 [===========>..................] - ETA: 1:11 - loss: 2.2794 - accuracy: 0.2054WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "10/10 [==============================] - 67s 6s/step - loss: 2.2776 - accuracy: 0.2232 - val_loss: 2.2747 - val_accuracy: 0.2462\n"
     ]
    }
   ],
   "source": [
    "# model training & storing\n",
    "fit_history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_TRAINING,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=STEPS_PER_EPOCH_VALIDATION,\n",
    "        callbacks=[checkpointer, early_stopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training checkpoint\n",
    "model.load_weights(\"./new_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.2764453887939453],\n",
       " 'accuracy': [0.23499999940395355],\n",
       " 'val_loss': [2.2746665477752686],\n",
       " 'val_accuracy': [0.2461538463830948]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the training metrics\n",
    "fit_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a test object\n",
    "test_generator = data_generator.flow_from_directory(\n",
    "    directory = TESING_DATASET,\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = TESTING_BATCH_SIZE,\n",
    "    class_mode = None,\n",
    "    shuffle = False,\n",
    "    seed = 123\n",
    ")\n",
    "\n",
    "test_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 19s 5s/step\n"
     ]
    }
   ],
   "source": [
    "# predict the test data\n",
    "prediction = model.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n",
    "predicted_class_indices = np.argmax(prediction, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<U><B>Point No. 4</B></U><br>\n",
    "Evaluation metrics i.e <B>Accuracy, Confusion Matrix, F1 Score, ROC-AUC Score</B> shall be calculated\n",
    "on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 1  0  0  0  0  0  0 12  0  1]\n",
      " [ 1  0  1  0  0  0  9  2  0  1]\n",
      " [ 1  2  0  0  0  0 10  1  0  0]\n",
      " [ 0  0  1  2  0  9  0  0  1  1]\n",
      " [ 2  0  0  0  3  1  0  8  0  0]\n",
      " [ 1  0  0  0  0  3  0  3  7  0]\n",
      " [ 0  0  0  0  0 13  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0]\n",
      " [ 1  0  0  0  2  0  0  0 11  0]\n",
      " [ 1  0  0  0  9  0  0  1  0  3]]\n",
      "Classification Report\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            beagle       0.12      0.07      0.09        14\n",
      "         chihuahua       0.00      0.00      0.00        14\n",
      "          doberman       0.00      0.00      0.00        14\n",
      "    french_bulldog       1.00      0.14      0.25        14\n",
      "  golden_retriever       0.21      0.21      0.21        14\n",
      "          malamute       0.12      0.21      0.15        14\n",
      "               pug       0.05      0.07      0.06        14\n",
      "     saint_bernard       0.34      1.00      0.51        14\n",
      "scottish_deerhound       0.58      0.79      0.67        14\n",
      "   tibetan_mastiff       0.50      0.21      0.30        14\n",
      "\n",
      "          accuracy                           0.27       140\n",
      "         macro avg       0.29      0.27      0.22       140\n",
      "      weighted avg       0.29      0.27      0.22       140\n",
      "\n",
      "ROC-AUC Score\n",
      "0.9305249227871562\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(prediction, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=selected_labels))\n",
    "\n",
    "print(\"ROC-AUC Score\")\n",
    "print(roc_auc_score(y_pred, prediction, multi_class='ovr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image path for testing\n",
    "image_path = \"./test/1c16315c9efe0ea92a38cdd20aa9d624.jpg\"\n",
    "\n",
    "# load image\n",
    "original = image.load_img(image_path, target_size=(224, 224))\n",
    "\n",
    "# convert into numpy array\n",
    "numpy_image = image.img_to_array(original)\n",
    "\n",
    "# expand dimension\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "\n",
    "# normalized image\n",
    "processed_image = preprocess_input(image_batch, mode='tf')\n",
    "\n",
    "# predict the image\n",
    "preds = model.predict(processed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beagle': 0.099908516,\n",
       " 'chihuahua': 0.09955728,\n",
       " 'doberman': 0.099856965,\n",
       " 'french_bulldog': 0.09963106,\n",
       " 'golden_retriever': 0.10047741,\n",
       " 'malamute': 0.099312104,\n",
       " 'pug': 0.1002064,\n",
       " 'saint_bernard': 0.09981777,\n",
       " 'scottish_deerhound': 0.100539885,\n",
       " 'tibetan_mastiff': 0.100692585}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(selected_labels, preds[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
